# ì„ í˜• íšŒê·€ (Linear Regression)

íšŒê·€(regression)ëŠ” **ìˆ«ì ê°’ì„ ì˜ˆì¸¡**í•˜ëŠ” ë° ì“°ì´ëŠ” ê°€ì¥ í”í•œ ì•Œê³ ë¦¬ì¦˜ ì§‘í•©ì„.  
íšŒê·€ ëª¨ë¸ì€ ê°€ëŠ¥í•œ ê°’ì´ ë¬´í•œíˆ ë§ì€ **ì—°ì†í˜• ì¶œë ¥**ì„ ì˜ˆì¸¡í•¨.

-   **ì„ í˜• íšŒê·€**: ë°ì´í„°ì— **ì§ì„ **ì„ ë§ì¶°ì„œ ì˜ˆì¸¡í•¨.
-   **ë¹„ì„ í˜• íšŒê·€**: ë°ì´í„°ì— **ê³¡ì„ **ì„ ë§ì¶°ì„œ ì˜ˆì¸¡í•¨.
-   **ë‹¨ë³€ëŸ‰(uniÂ­variate) ì„ í˜• íšŒê·€**: ì…ë ¥ ë³€ìˆ˜ê°€ **í•˜ë‚˜**ì¼ ë•Œì˜ ì„ í˜• íšŒê·€.  
    (ì‹¤ì „ì—ì„œëŠ” ë°© ê°œìˆ˜, ìœ„ì¹˜, ì—°ì‹ ë“± **ì—¬ëŸ¬ ì…ë ¥ ë³€ìˆ˜**ë¥¼ ë™ì‹œì— ì“°ëŠ” ë‹¤ë³€ëŸ‰ ëª¨ë¸ë„ í”í•¨.)

ì˜ˆ: **ì£¼íƒ ê°€ê²© ì˜ˆì¸¡**  
ì£¼íƒì˜ **ë©´ì (ì œê³±í”¼íŠ¸)** ì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„, ê·¸ì— ë”°ë¥¸ **ê°€ê²©**ì„ ì˜ˆì¸¡í•˜ëŠ” ë¬¸ì œëŠ” ì „í˜•ì ì¸ ì„ í˜• íšŒê·€ ì˜ˆì‹œì„.

![Untitled](Untitled.png)

---

# ëª¨ë¸ì˜ ì •ì˜ (Defining the Model)

ì„ í˜• íšŒê·€ ëª¨ë¸ì€ ë‹¤ìŒ í•¨ìˆ˜ë¡œ í‘œí˜„ë¨:

$$
f_{w,b}(x) = wx + b
$$

ì—¬ê¸°ì„œ

-   $w$ : ì§ì„ ì˜ **ê¸°ìš¸ê¸°**(slope) â€” $x$ ê°€ 1ë§Œí¼ ëŠ˜ ë•Œ $y$ ê°€ ì–¼ë§ˆë‚˜ ë³€í•˜ëŠ”ì§€
-   $b$ : **ì ˆí¸**(y-intercept) â€” $x = 0$ ì¼ ë•Œì˜ ì˜ˆì¸¡ê°’

í•™ìŠµì´ ëë‚˜ ì§ì„ ì´ ë°ì´í„°ì— **ì í•©(fit)** ë˜ë©´, ê°™ì€ í˜•íƒœì˜ í•¨ìˆ˜ë¡œ **ìƒˆ ì…ë ¥**ì— ëŒ€í•´ ê°’ì„ ì˜ˆì¸¡í•¨.  
ë‹¤ë§Œ ì´ì œ ì¶œë ¥ì€ â€œì •ë‹µâ€ì´ ì•„ë‹ˆë¼ **ëª¨ë¸ì´ ì¶”ì •í•œ ê°’(ì˜ˆì¸¡ê°’)** ì´ ë¨.

ì˜ˆì¸¡ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŒ:

$$
\hat{y} = f_{w,b}(x^i) = wx^i + b
$$

-   $x^i$ : $i$ ë²ˆì§¸ ì…ë ¥(ì˜ˆ: $i$ ë²ˆì§¸ ì§‘ì˜ ë©´ì )
-   $\hat{y}$ : ê·¸ ì…ë ¥ì— ëŒ€í•œ **ì˜ˆì¸¡ ê°€ê²©**

ìš”ì•½: ë°ì´í„°ë¥¼ ê°€ì¥ ì˜ ì„¤ëª…í•˜ëŠ” ì§ì„  $wx + b$ ë¥¼ ì°¾ê³ ,  
ê·¸ ì§ì„ ì„ ì´ìš©í•´ **ìƒˆë¡œìš´ $x$** ì— ëŒ€í•œ $\hat{y}$ ë¥¼ ê³„ì‚°í•˜ëŠ” ê²ƒì´ ì„ í˜• íšŒê·€ì˜ í•µì‹¬ì„.

# ë¹„ìš© í•¨ìˆ˜ (The Cost Function)

## ë¹„ìš© í•¨ìˆ˜ì˜ ê°œë… (Finding the Cost Function)

ì„ í˜• íšŒê·€ì—ì„œ **ê¸°ìš¸ê¸°($w$)** ì™€ **ì ˆí¸($b$)** ì€ íšŒê·€ì„ ì˜ **ê¸°ìš¸ê¸° ë°©í–¥ê³¼ ìœ„ì¹˜**ë¥¼ ê²°ì •í•˜ëŠ” í•µì‹¬ ìš”ì†Œë‹¤.  
ë”°ë¼ì„œ ì´ ë‘ íŒŒë¼ë¯¸í„°ê°€ ë°ì´í„°ì— **ì–¼ë§ˆë‚˜ ì˜ ë§ëŠ”ì§€**ë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ í‰ê°€í•˜ëŠ” ê²ƒì´ ë§¤ìš° ì¤‘ìš”í•˜ë‹¤.  
ì´ë ‡ê²Œ ëª¨ë¸ì˜ â€œì í•©ë„(fit)â€ë¥¼ ìˆ˜ì¹˜í™”í•˜ëŠ” ê³¼ì •ì„ **ë¹„ìš© í•¨ìˆ˜(cost function)** ë¥¼ ì°¾ëŠ” ê³¼ì •ì´ë¼ê³  í•œë‹¤.

---

ì˜ˆì¸¡ê°’ $\hat{y}^i$ ì™€ ì‹¤ì œê°’ $y^i$ ì‚¬ì´ì˜ ì°¨ì´ë¥¼ **ì˜¤ì°¨(error)** ë¼ê³  í•˜ë©°,  
ì´ ë‘ ê°’ì˜ ì°¨ì´ë¥¼ ì œê³±í•œ í˜•íƒœë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$
\sum_{i=1}^m (\hat{y}^i - y^i)^2
$$

-   $i$: ê°œë³„ í•™ìŠµ ë°ì´í„°ì˜ ì¸ë±ìŠ¤
-   $m$: ì „ì²´ í•™ìŠµ ë°ì´í„°ì˜ ê°œìˆ˜
-   $(\hat{y}^i - y^i)^2$: ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì°¨ì´ë¥¼ ì œê³±í•œ ì˜¤ì°¨ (ì œê³± ì˜¤ì°¨)

ì´ ì‹ì€ **ëª¨ë“  í•™ìŠµ ë°ì´í„°ì˜ ì œê³± ì˜¤ì°¨ë¥¼ ë”í•œ ê°’**ìœ¼ë¡œ,  
ëª¨ë¸ì´ ì „ì²´ ë°ì´í„°ì— ì–¼ë§ˆë‚˜ ì˜ ë§ëŠ”ì§€ë¥¼ ë³´ì—¬ì¤€ë‹¤.

---

### í‰ê·  ì œê³± ì˜¤ì°¨ (Mean Squared Error)

ë°ì´í„°ê°€ ë§ì•„ì§ˆìˆ˜ë¡ ë‹¨ìˆœíˆ ì˜¤ì°¨ì˜ í•©ì„ ê³„ì‚°í•˜ê¸°ë³´ë‹¤  
**í‰ê·  ì œê³± ì˜¤ì°¨(Mean Squared Error, MSE)** ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ íš¨ìœ¨ì ì´ë‹¤.

ì´ë¥¼ ìˆ˜ì‹ìœ¼ë¡œ ì •ì˜í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$
J(w,b) = \frac{1}{2m} \sum_{i=1}^m (\hat{y}^i - y^i)^2
$$

ì—¬ê¸°ì„œ

-   $J(w,b)$: ë¹„ìš© í•¨ìˆ˜ (Cost Function)
-   $w, b$: í•™ìŠµí•´ì•¼ í•  íŒŒë¼ë¯¸í„° (ê¸°ìš¸ê¸°ì™€ ì ˆí¸)
-   $\frac{1}{2m}$: í‰ê· ì„ ë‚´ê¸° ìœ„í•œ ì •ê·œí™” í•­(ë‹¨, 2ëŠ” ìˆ˜í•™ì  í¸ì˜ìƒ ì¶”ê°€ë¨)

> ì°¸ê³ : $2m$ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ì´ìœ ëŠ” ë¯¸ë¶„ ì‹œ ê³„ì‚°ì„ ë‹¨ìˆœí™”í•˜ê¸° ìœ„í•´ì„œì´ë©°,  
> ê²°ê³¼ì ìœ¼ë¡œ í•™ìŠµ ê³¼ì •ì˜ ì•ˆì •ì„±ì„ ë†’ì—¬ì¤€ë‹¤.  
> $m$ë§Œ ì‚¬ìš©í•´ë„ ìˆ˜í•™ì ìœ¼ë¡œëŠ” ë™ì¼í•œ ì˜ë¯¸ë¥¼ ê°€ì§„ë‹¤.

---

### í•¨ìˆ˜ í˜•íƒœë¡œì˜ ì¼ë°˜í™”

ì•ì„œ ì‚¬ìš©í•œ $\hat{y}^i$ ëŒ€ì‹ ,  
ì´ë¥¼ **í•¨ìˆ˜ í˜•íƒœì˜ ëª¨ë¸ í‘œí˜„**ìœ¼ë¡œ ëŒ€ì²´í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$
J(w,b) = \frac{1}{2m} \sum_{i=1}^m (f_{w,b}(x^i) - y^i)^2
$$

ì´ ì‹ì—ì„œ

-   $f_{w,b}(x^i)$: ì…ë ¥ $x^i$ì— ëŒ€í•œ ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ (í•¨ìˆ˜ í˜•íƒœ)
-   $y^i$: ì‹¤ì œ ê´€ì¸¡ê°’ (ì •ë‹µ)
-   $w, b$: ëª¨ë¸ì´ í•™ìŠµì„ í†µí•´ ì¡°ì •í•´ì•¼ í•  ë§¤ê°œë³€ìˆ˜(parameter)

ì¦‰, **ë¹„ìš© í•¨ìˆ˜ $J(w,b)$** ëŠ”  
ëª¨ë¸ì´ ëª¨ë“  ë°ì´í„°ì— ëŒ€í•´ ì–¼ë§ˆë‚˜ ì˜ ì˜ˆì¸¡í•˜ê³  ìˆëŠ”ì§€ë¥¼ ìˆ˜ì¹˜ë¡œ í‰ê°€í•˜ë©°,  
ì´ ê°’ì´ ì‘ì„ìˆ˜ë¡ ëª¨ë¸ì´ ë°ì´í„°ì— ë” ì˜ ë§ëŠ”ë‹¤ëŠ” ëœ»ì´ë‹¤.

---

### í•µì‹¬ ìš”ì•½

-   ë¹„ìš© í•¨ìˆ˜ëŠ” ëª¨ë¸ì˜ **ì˜ˆì¸¡ ì˜¤ì°¨ë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ í‰ê°€**í•˜ê¸° ìœ„í•œ ì²™ë„ì´ë‹¤.
-   **ì œê³± ì˜¤ì°¨(Squared Error)** ë¥¼ ì‚¬ìš©í•˜ì—¬ í° ì˜¤ì°¨ì— ë” í° íŒ¨ë„í‹°ë¥¼ ë¶€ì—¬í•œë‹¤.
-   $\frac{1}{2m}$ ì„ ê³±í•´ í‰ê· í™”í•¨ìœ¼ë¡œì¨,  
    ë°ì´í„° ê°œìˆ˜ì— ê´€ê³„ì—†ì´ ì¼ê´€ëœ í•™ìŠµì´ ê°€ëŠ¥í•˜ë‹¤.
-   ëª¨ë¸ì€ í•™ìŠµì„ í†µí•´ **ë¹„ìš© í•¨ìˆ˜ $J(w,b)$ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥**ìœ¼ë¡œ  
    $w$ì™€ $b$ë¥¼ ì ì§„ì ìœ¼ë¡œ ì¡°ì •í•˜ê²Œ ëœë‹¤.

## ë¹„ìš© í•¨ìˆ˜ì˜ ìµœì í™” (Optimizing the Cost Function)

ì„ í˜• íšŒê·€ì˜ í•µì‹¬ ëª©í‘œëŠ” **ë¹„ìš© í•¨ìˆ˜ $J(w,b)$ ë¥¼ ê°€ëŠ¥í•œ í•œ ì‘ê²Œ(minimize)** ë§Œë“œëŠ” ê²ƒì´ë‹¤.  
ì¦‰, ëª¨ë¸ì˜ ì˜ˆì¸¡ì´ ì‹¤ì œ ë°ì´í„°ì™€ ìµœëŒ€í•œ ê°€ê¹ê²Œ ë˜ë„ë¡  
ë§¤ê°œë³€ìˆ˜ $w$ (ê¸°ìš¸ê¸°)ì™€ $b$ (ì ˆí¸)ì„ ì¡°ì •í•˜ëŠ” ê³¼ì •ì´ë‹¤.

ì´ë¥¼ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$
\underset{w,b}{\text{minimize}} \; J(w,b)
$$

ì¦‰,

> â€œ$w$ì™€ $b$ë¥¼ ì¡°ì •í•˜ì—¬ ë¹„ìš© í•¨ìˆ˜ $J(w,b)$ì˜ ê°’ì„ ìµœì†Œí™”í•˜ëŠ” ì¡°í•©ì„ ì°¾ì•„ë¼.â€

---

### ë¹„ìš© í•¨ìˆ˜ì˜ ì‹œê°í™”

ë¹„ìš© í•¨ìˆ˜ $J(w,b)$ëŠ” ë‘ ê°œì˜ ë³€ìˆ˜($w$, $b$)ë¥¼ í¬í•¨í•˜ë¯€ë¡œ,  
ì´ë¥¼ **2ì°¨ì› í‰ë©´**ì´ë‚˜ **3ì°¨ì› ê³µê°„**ìœ¼ë¡œ ì‹œê°í™”í•  ìˆ˜ ìˆë‹¤.

-   **2D ê·¸ë˜í”„ (Contour Plot)** :  
    ë™ì¼í•œ ë¹„ìš©ê°’ì„ ê°€ì§€ëŠ” ì ë“¤ì„ ë“±ê³ ì„ ì²˜ëŸ¼ ì—°ê²°í•œ ê·¸ë˜í”„  
    â†’ ë¹„ìš©ì´ ë‚®ì„ìˆ˜ë¡ ì¤‘ì•™ìœ¼ë¡œ ëª¨ì„

-   **3D ê·¸ë˜í”„ (Surface Plot)** :  
    $w$ì™€ $b$ì˜ ë³€í™”ì— ë”°ë¼ ë¹„ìš©ì´ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ë¥¼ ì…ì²´ì ìœ¼ë¡œ í‘œí˜„  
    â†’ ìµœì†Ÿê°’ì€ â€œê·¸ë¦‡ì˜ ë°”ë‹¥â€ì²˜ëŸ¼ ë³´ì´ëŠ” ì§€ì 

![Surface graph of the cost function; plotted against *w* and *b.*](Untitled%201.png)

_ë¹„ìš© í•¨ìˆ˜ì˜ 3D í‘œë©´ ê·¸ë˜í”„ â€” $w$ì™€ $b$ì— ë”°ë¥¸ ë³€í™”_

![Contour graph of the cost function; plotted against *w* and *b.*](Screenshot_2023-03-27_212400.png)

_ë¹„ìš© í•¨ìˆ˜ì˜ 2D ë“±ê³ ì„  ê·¸ë˜í”„ â€” $w$ì™€ $b$ì˜ ìµœì  ì¡°í•©ì€ ì¤‘ì•™ ì§€ì ì—ì„œ ê²°ì •ë¨_

---

# ë‹¤ì¤‘ íŠ¹ì„± ì„ í˜• íšŒê·€ (Multiple Feature Linear Regression)

ì§€ê¸ˆê¹Œì§€ëŠ” ì…ë ¥ê°’ì´ í•˜ë‚˜ì¸ **ë‹¨ì¼ íŠ¹ì„±(single feature)** ì„ í˜• íšŒê·€ë¥¼ ë‹¤ë£¨ì—ˆë‹¤.  
í•˜ì§€ë§Œ ì‹¤ì œ ë¬¸ì œì—ì„œëŠ” í•˜ë‚˜ì˜ ë³€ìˆ˜ë¡œ ê²°ê³¼ë¥¼ ì •í™•íˆ ì˜ˆì¸¡í•˜ê¸° ì–´ë µë‹¤.

ì˜ˆë¥¼ ë“¤ì–´, ì§‘ê°’ì„ ì˜ˆì¸¡í•  ë•ŒëŠ” **ë©´ì **ë§Œìœ¼ë¡œëŠ” ì¶©ë¶„í•˜ì§€ ì•Šë‹¤.  
ë‹¤ìŒê³¼ ê°™ì€ ì—¬ëŸ¬ ìš”ì¸ì´ í•¨ê»˜ ì‘ìš©í•œë‹¤.

-   ë°©ì˜ ê°œìˆ˜ (number of bedrooms)
-   ìš•ì‹¤ì˜ ê°œìˆ˜ (number of bathrooms)
-   ìœ„ì¹˜(location)
-   ê±´ì¶• ì—°ë„(year built) ë“±

ì´ì²˜ëŸ¼ ì—¬ëŸ¬ ì…ë ¥ ë³€ìˆ˜ë¥¼ í•¨ê»˜ ê³ ë ¤í•˜ëŠ” ëª¨ë¸ì„ **ë‹¤ì¤‘ ì„ í˜• íšŒê·€ (Multiple Linear Regression)** ë¼ê³  í•œë‹¤.

---

## ìˆ˜ì‹ í‘œê¸°ë²• (Notation)

ì…ë ¥ íŠ¹ì„±ì´ ì—¬ëŸ¬ ê°œì¸ ê²½ìš°, í‘œê¸°ë²•ì„ ì•½ê°„ ìˆ˜ì •í•´ì•¼ í•œë‹¤.

| ê¸°í˜¸        | ì˜ë¯¸                                                                    |
| ----------- | ----------------------------------------------------------------------- |
| $x_j$       | $j$ë²ˆì§¸ íŠ¹ì„± (feature)                                                  |
| $n$         | ì „ì²´ íŠ¹ì„±ì˜ ê°œìˆ˜                                                        |
| $\vec{x}^i$ | $i$ë²ˆì§¸ í•™ìŠµ ë°ì´í„°ì˜ ëª¨ë“  íŠ¹ì„± ë²¡í„° (ì˜ˆ: $[x_1^i, x_2^i, ..., x_n^i]$) |
| ${x_j}^i$   | $i$ë²ˆì§¸ í•™ìŠµ ë°ì´í„°ì—ì„œ $j$ë²ˆì§¸ íŠ¹ì„±ì˜ ê°’                               |

---

ì˜ˆë¥¼ ë“¤ì–´, ì§‘ê°’ ì˜ˆì¸¡ ë¬¸ì œì—ì„œ  
í•œ ë°ì´í„°($i$ë²ˆì§¸ ì§‘)ì˜ ì…ë ¥ ë²¡í„°ëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.

$$
\vec{x}^i = [ \text{ë©´ì }, \text{ë°© ê°œìˆ˜}, \text{ìš•ì‹¤ ê°œìˆ˜}, \text{ìœ„ì¹˜}, \ldots ]
$$

ì¦‰, ë‹¨ì¼ ë³€ìˆ˜ $x$ ëŒ€ì‹   
ë²¡í„° í˜•íƒœì˜ ì…ë ¥ $\vec{x}$ ë¥¼ ì‚¬ìš©í•¨ìœ¼ë¡œì¨  
ì—¬ëŸ¬ ìš”ì¸ì´ ê²°ê³¼ê°’ì— ë™ì‹œì— ì˜í–¥ì„ ì£¼ë„ë¡ ëª¨ë¸ì„ í™•ì¥í•œ ê²ƒì´ë‹¤.

---

### í•µì‹¬ ìš”ì•½

-   ë¹„ìš© í•¨ìˆ˜ $J(w,b)$ ëŠ” ëª¨ë¸ì˜ ì˜ˆì¸¡ ì •í™•ë„ë¥¼ í‰ê°€í•˜ëŠ” ê¸°ì¤€ì„.
-   $w$, $b$ ë¥¼ ì¡°ì •í•˜ì—¬ $J(w,b)$ ê°€ **ê°€ì¥ ì‘ì•„ì§€ëŠ” ì§€ì **ì„ ì°¾ëŠ” ê²ƒì´ ëª©í‘œì„.
-   2D/3D ê·¸ë˜í”„ë¥¼ í†µí•´ ë¹„ìš© í•¨ìˆ˜ì˜ **ëª¨ì–‘(ë³¼ë¡í•œ í˜•íƒœ)** ê³¼ **ìµœì†Ÿê°’ ìœ„ì¹˜**ë¥¼ ì‹œê°í™”í•  ìˆ˜ ìˆìŒ.
-   ì…ë ¥ ë³€ìˆ˜ê°€ ì—¬ëŸ¬ ê°œì¸ ê²½ìš°, ì„ í˜• íšŒê·€ëŠ” **ë²¡í„° í˜•íƒœ**ë¡œ í™•ì¥ë˜ì–´  
    ì—¬ëŸ¬ íŠ¹ì„±(feature)ì„ ë™ì‹œì— ê³ ë ¤í•  ìˆ˜ ìˆê²Œ ë¨.

## ëª¨ë¸ (Model)

ì…ë ¥ ë³€ìˆ˜ê°€ ì—¬ëŸ¬ ê°œë¡œ ëŠ˜ì–´ë‚˜ë©´, ëª¨ë¸ í•¨ìˆ˜ ì—­ì‹œ ì´ì— ë§ê²Œ í™•ì¥ë˜ì–´ì•¼ í•œë‹¤.  
ì—¬ê¸°ì„œ **$n$** ì€ ì…ë ¥ íŠ¹ì„±(feature)ì˜ ê°œìˆ˜ë¥¼ ì˜ë¯¸í•œë‹¤.

---

### ğŸ”¹ ê¸°ì¡´ ë‹¨ì¼ ë³€ìˆ˜ ëª¨ë¸ (Single Variable Model)

$$
f_{w,b}(x) = wx + b
$$

---

### ğŸ”¹ ë‹¤ì¤‘ ë³€ìˆ˜ ëª¨ë¸ (Multivariable Model)

$$
f_{w,b}(x) = w_1x_1 + w_2x_2 + \ldots + w_nx_n + b
$$

ì¦‰, ì…ë ¥ê°’ì´ ì—¬ëŸ¬ ê°œì¼ ë•ŒëŠ” ê° íŠ¹ì„±ë§ˆë‹¤ ì„œë¡œ ë‹¤ë¥¸ ê°€ì¤‘ì¹˜($w_j$)ê°€ ê³±í•´ì§„ í›„  
ëª¨ë‘ ë”í•´ì§€ê³ , ë§ˆì§€ë§‰ì— ì ˆí¸ $b$ ê°€ ë”í•´ì§„ë‹¤.

---

### ğŸ”¹ ë²¡í„° í‘œí˜„ (Vector Form)

í‘œê¸°ë²•ì„ ë‹¨ìˆœí™”í•˜ê¸° ìœ„í•´, ìœ„ì˜ ëª¨ë¸ì„ **ë²¡í„° í˜•íƒœ**ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.  
ë‹¨, $b$ ëŠ” ë‹¨ìˆœí•œ ìƒìˆ˜ì´ë¯€ë¡œ ë²¡í„°ì—ëŠ” í¬í•¨ë˜ì§€ ì•ŠëŠ”ë‹¤.

$$
\vec{w} =
\begin{bmatrix}
w_1 & w_2 & w_3 & \dots & w_n
\end{bmatrix}
$$

$$
\vec{x} =
\begin{bmatrix}
x_1 & x_2 & x_3 & \dots & x_n
\end{bmatrix}
$$

ì´ë•Œ, ìµœì¢… ëª¨ë¸ì€ ë‹¤ìŒê³¼ ê°™ì€ **ë‚´ì (dot product)** í˜•íƒœë¡œ ì •ë¦¬ëœë‹¤.

$$
f_{\vec{w},b}(\vec{x}) = \vec{w} \cdot \vec{x} + b
$$

---

## ë²¡í„°í™” (Vectorization)

ë²¡í„°í™”ëŠ” ì„ í˜•ëŒ€ìˆ˜ í˜•íƒœì˜ ìˆ˜ì‹ì„ ì»´í“¨í„°ê°€ ë¹ ë¥´ê²Œ ê³„ì‚°í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì´ë‹¤.  
ì´ ê¸°ë²•ì„ ì‚¬ìš©í•˜ë©´ ë°˜ë³µë¬¸ ì—†ì´ë„ ëŒ€ê·œëª¨ ì—°ì‚°ì„ ë¹ ë¥´ê²Œ ìˆ˜í–‰í•  ìˆ˜ ìˆìœ¼ë©°,  
GPUë‚˜ CPUì˜ **ë³‘ë ¬ ì—°ì‚°(parallel computation)** ì„ ì ê·¹ì ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆë‹¤.

ë˜í•œ íŒŒì´ì¬ì—ì„œëŠ” [NumPy](https://numpy.org/) ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´  
ì´ëŸ¬í•œ ë²¡í„°í™” ì—°ì‚°ì„ ê°„ë‹¨í•˜ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤.

---

### ğŸ”¹ ë³€ìˆ˜ ë° ë§¤ê°œë³€ìˆ˜ ì •ì˜

ë‹¤ìŒ ì˜ˆì‹œëŠ” ì…ë ¥ íŠ¹ì„±ì´ 3ê°œì¸ ê²½ìš°ë¥¼ ê°€ì •í•œë‹¤.

$$
n = 3
$$

$$
\vec{w} = [w_1, w_2, w_3], \quad b = 4, \quad \vec{x} = [x_1, x_2, x_3]
$$

```python
import numpy as np

w = np.array([1.0, 2.5, -3.3])
b = 4
x = np.array([10, 20, 30])
```

---

## ë²¡í„°í™”ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš° (Without Vectorization - NumPy ë¯¸ì‚¬ìš©)

ë°˜ë³µë¬¸(`for loop`)ì„ ì‚¬ìš©í•´ ì§ì ‘ í•­ëª©ë³„ ê³±ì…ˆì„ ìˆ˜í–‰í•˜ëŠ” ë°©ì‹ì´ë‹¤.  
ê° íŠ¹ì„±(feature)ì— ëŒ€í•œ ê°€ì¤‘ì¹˜($w_j$)ì™€ ì…ë ¥ê°’($x_j$)ì„ ê³±í•œ í›„ ëª¨ë‘ ë”í•˜ê³ , ë§ˆì§€ë§‰ìœ¼ë¡œ ì ˆí¸($b$)ì„ ë”í•œë‹¤.

$$
f_{\vec{w},b}(\vec{x}) = \left( \sum_{j=1}^{n} w_j x_j \right) + b
$$

```python
f = 0
for j in range(n):  # 0 ~ n-1
    f += w[j] * x[j]
f += b
```

ì´ ë°©ì‹ì€ ì§ê´€ì ì´ì§€ë§Œ,
íŠ¹ì„±ì˜ ê°œìˆ˜($n$)ê°€ ì»¤ì§ˆìˆ˜ë¡ ë°˜ë³µ íšŸìˆ˜ê°€ ë§ì•„ì ¸ ë¹„íš¨ìœ¨ì ì´ë‹¤.

---

## ë²¡í„°í™”ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš° (With Vectorization - NumPy í™œìš©)

ë²¡í„°í™”ë¥¼ ì ìš©í•˜ë©´ ë‹¨ í•œ ì¤„ë¡œ ê°™ì€ ì—°ì‚°ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.
ì´ë•Œ NumPyì˜ np.dot() í•¨ìˆ˜ëŠ” ë²¡í„° $\vec{w}$ ì™€ $\vec{x}$ ì˜ ë‚´ì (dot product) ì„ ê³„ì‚°í•œë‹¤.

$$
f_{\vec{w},b}(\vec{x}) = \vec{w} \cdot \vec{x} + b
$$

```python
f = np.dot(w,x) + b
```

ë²¡í„°í™”ì˜ ê°€ì¥ í° ì¥ì ì€ ì½”ë“œì˜ ë‹¨ìˆœí™”ì™€ ì„±ëŠ¥ í–¥ìƒì´ë‹¤.
ì´ëŠ” NumPyê°€ ë‚´ë¶€ì ìœ¼ë¡œ ë³‘ë ¬ ì—°ì‚°(parallel computation) ì„ ì§€ì›í•˜ê¸° ë•Œë¬¸ì´ë‹¤.

â¸»

ë²¡í„°í™”ì˜ ì‘ë™ ë°©ì‹ ë¹„êµ

| êµ¬ë¶„                | ê³„ì‚° ë°©ì‹                                  | íŠ¹ì§•                   |
| ------------------- | ------------------------------------------ | ---------------------- |
| ë¹„ë²¡í„°í™” (for loop) | ê° í•­ëª© $w_j x_j$ ì„ ìˆœì°¨ì ìœ¼ë¡œ ë”í•¨       | ì„ í˜•ì  (linear) ê³„ì‚°   |
| ë²¡í„°í™” (NumPy)      | ëª¨ë“  í•­ëª© $w_j x_j$ ì„ ë™ì‹œì— ê³„ì‚° í›„ í•©ì‚° | ë³‘ë ¬ì  (parallel) ê³„ì‚° |

ë²¡í„°í™”ë¥¼ ì‚¬ìš©í•˜ë©´ ì½”ë“œê°€ ë‹¨ìˆœí•˜ê³  ê¹”ë”í•´ì§ˆ ë¿ë§Œ ì•„ë‹ˆë¼  
**ì‹¤í–‰ ì†ë„ë„ í›¨ì”¬ ë¹¨ë¼ì§„ë‹¤.**  
ê·¸ ì´ìœ ëŠ” **NumPyê°€ ë‚´ë¶€ì ìœ¼ë¡œ ì»´í“¨í„°ì˜ ë³‘ë ¬ ì—°ì‚° í•˜ë“œì›¨ì–´(parallel hardware)** ë¥¼ í™œìš©í•˜ê¸° ë•Œë¬¸ì´ë‹¤.

---

ì˜ˆë¥¼ ë“¤ì–´, ë²¡í„°í™”ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì€ ê²½ìš°ë¥¼ ì‚´í´ë³´ì.  
`for` ë°˜ë³µë¬¸ì€ ê° ë§¤ê°œë³€ìˆ˜ $j$ ì— ëŒ€í•´  
$w_j$ ì™€ $x_j$ ì˜ ê³±ì„ **í•˜ë‚˜ì”© ìˆœì„œëŒ€ë¡œ(linearly)** ê³„ì‚°í•œë‹¤.

ì¦‰, ë‹¤ìŒê³¼ ê°™ì´ ë‹¨ê³„ë³„ë¡œ ëˆ„ì í•©ì„ êµ¬í•œë‹¤.

```python
f = 0
for j in range(n):  # 0ë¶€í„° n-1ê¹Œì§€
    f += w[j] * x[j]
f += b
```

ì´ ë°©ì‹ì€ ì…ë ¥ íŠ¹ì„±(feature) ì˜ ê°œìˆ˜ $n$ì´ ë§ì„ìˆ˜ë¡
ì—°ì‚° ì†ë„ê°€ ì„ í˜•ì ìœ¼ë¡œ ëŠë ¤ì§„ë‹¤.

â¸»

ë°˜ë©´, ë²¡í„°í™”(vectorization) ë¥¼ ì‚¬ìš©í•˜ë©´
ëª¨ë“  $w_j$ ì™€ $x_j$ ì˜ ê³±ì´ ë™ì‹œì—(parallel) ê³„ì‚°ëœë‹¤.
NumPyëŠ” GPUë‚˜ CPUì˜ ë³‘ë ¬ ì—°ì‚° ê¸°ëŠ¥ì„ ì´ìš©í•´
ì—¬ëŸ¬ ê³±ì…ˆì„ í•œ ë²ˆì— ì²˜ë¦¬í•˜ê³ , ê·¸ ê²°ê³¼ë¥¼ ë”í•´ $f$ ê°’ì„ êµ¬í•œë‹¤.

```python
f = np.dot(w, x) + b
```

â¸»

ì¦‰,

-   ë¹„ë²¡í„°í™”: ìˆœì„œëŒ€ë¡œ í•˜ë‚˜ì”© ê³„ì‚° â†’ ëŠë¦¼
-   ë²¡í„°í™”: ë™ì‹œì— ë³‘ë ¬ë¡œ ê³„ì‚° â†’ ë§¤ìš° ë¹ ë¦„

ê²°ê³¼ì ìœ¼ë¡œ ë²¡í„°í™”ë¥¼ ì ìš©í•˜ë©´
ì½”ë“œëŠ” ë” ê°„ê²°í•´ì§€ê³ , ì‹¤í–‰ ì†ë„ëŠ” íšê¸°ì ìœ¼ë¡œ í–¥ìƒëœë‹¤.

# ë‹¤ì¤‘ ì„ í˜• íšŒê·€ì˜ ê²½ì‚¬ í•˜ê°•ë²• (Gradient Descent for Multiple Linear Regression)

ì—¬ëŸ¬ ê°œì˜ ì…ë ¥ ë³€ìˆ˜(íŠ¹ì§•, feature)ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ì—ë„  
ê²½ì‚¬ í•˜ê°•ë²•(Gradient Descent)ì˜ ì›ë¦¬ëŠ” ë™ì¼í•˜ë‹¤.  
ë‹¨ì§€, **ë²¡í„° í‘œê¸°(vector notation)** ë¥¼ ì´ìš©í•´ ìˆ˜ì‹ì„ í™•ì¥í•˜ë©´ ëœë‹¤.

---

### ğŸ”¹ ê²½ì‚¬ í•˜ê°•ë²•ì˜ ê¸°ë³¸ í˜•íƒœ (Pre-derived Gradient Descent Algorithm)

ê²½ì‚¬ í•˜ê°•ë²•ì€ **ë¹„ìš© í•¨ìˆ˜ $J(\vec{w}, b)$** ë¥¼ ìµœì†Œí™”í•˜ê¸° ìœ„í•´  
ê°€ì¤‘ì¹˜ $w$ ì™€ í¸í–¥ $b$ ë¥¼ ë°˜ë³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ë‹¤.

$$
w = w - \alpha \frac{\partial}{\partial w} J(\vec{w},b)
$$

$$
b = b - \alpha \frac{\partial}{\partial b} J(\vec{w},b)
$$

ì—¬ê¸°ì„œ $\alpha$ ëŠ” **í•™ìŠµë¥ (learning rate)** ë¡œ,  
í•œ ë²ˆì˜ ì—…ë°ì´íŠ¸ì—ì„œ ì´ë™í•˜ëŠ” í¬ê¸°ë¥¼ ê²°ì •í•œë‹¤.

---

### ğŸ”¹ ìµœì¢… ê²½ì‚¬ í•˜ê°•ì‹ (Final Gradient Descent Algorithm)

ì´ë¥¼ ì—¬ëŸ¬ ê°œì˜ íŠ¹ì„±(feature)ì„ ê°€ì§„ ë°ì´í„°ì…‹ìœ¼ë¡œ í™•ì¥í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$
w_j = w_j - \alpha \frac{1}{m} \sum_{i=1}^m \left( f_{\vec{w},b}(\vec{x}^i) - y^i \right)x_j^{(i)}
$$

$$
b = b - \alpha \frac{1}{m} \sum_{i=1}^m \left( f_{\vec{w},b}(\vec{x}^i) - y^i \right)
$$

> ì¦‰, ëª¨ë“  ê°€ì¤‘ì¹˜ $w_j$ ($j = 1, \dots, n$)ì™€  
> í¸í–¥ $b$ ë¥¼ **ë™ì‹œì—(simultaneously)** ì—…ë°ì´íŠ¸í•œë‹¤.

---

### ğŸ”¹ ì£¼ì˜í•  ì 

-   $b$ ëŠ” ë‹¨ì¼ ê°’ì´ë¯€ë¡œ,  
    $n$ ê°œì˜ íŠ¹ì„±(feature)ì— ëŒ€í•´ ê°ê° ê°±ì‹ í•  í•„ìš”ê°€ ì—†ë‹¤.
-   $w_j$ ì˜ ê²½ìš°, ê° íŠ¹ì„± $x_j^{(i)}$ ì— ëŒ€í•´  
    ì˜¤ì°¨ í•­ $(f_{\vec{w},b}(\vec{x}^i) - y^i)$ ì„ ê³±í•´ ì—…ë°ì´íŠ¸í•œë‹¤.  
    ì´ë•Œ $x^i$ ëŠ” í˜„ì¬ ìƒ˜í”Œ $i$ ì— ëŒ€í•œ ì…ë ¥ ë²¡í„°ë¥¼ ì˜ë¯¸í•œë‹¤.

---

# ì •ê·œí™”ëœ ì„ í˜• íšŒê·€ (Regularized Linear Regression)

ì •ê·œí™”(Regularization)ëŠ” ëª¨ë¸ì´ **ê³¼ì í•©(overfitting)** ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´  
ê°€ì¤‘ì¹˜ $w$ ì˜ í¬ê¸°ë¥¼ ì œì–´í•˜ëŠ” ê¸°ë²•ì´ë‹¤.

ì •ê·œí™” í•­ì´ ì¶”ê°€ë˜ë©´, ë¹„ìš© í•¨ìˆ˜ì™€ ê²½ì‚¬ í•˜ê°•ì‹ ëª¨ë‘ ì¼ë¶€ ìˆ˜ì •ì´ í•„ìš”í•˜ë‹¤.

---

### ğŸ”¹ ì •ê·œí™”ê°€ í¬í•¨ëœ ë¹„ìš© í•¨ìˆ˜ (Regularized Cost Function)

ì •ê·œí™”ëœ ì„ í˜• íšŒê·€ì˜ ë¹„ìš© í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

$$
J(\vec{w}, b)
= \frac{1}{2m} \sum_{i=1}^{m}
\Big( f_{\vec{w},b}(\vec{x}^{(i)}) - y^{(i)} \Big)^2
+ \frac{\lambda}{2m} \sum_{j=1}^{n} w_j^2
$$

ì—¬ê¸°ì„œ

-   $\lambda$ : ì •ê·œí™” ê³„ìˆ˜ (regularization parameter)
-   $w_j$ : ê° íŠ¹ì„±(feature)ì— ëŒ€í•œ ê°€ì¤‘ì¹˜

ì •ê·œí™” í•­ $\frac{\lambda}{2m} \sum w_j^2$ ëŠ”  
ê°€ì¤‘ì¹˜ê°€ ë„ˆë¬´ ì»¤ì§€ëŠ” ê²ƒì„ ì–µì œí•˜ì—¬,  
ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ì—ë§Œ ê³¼ë„í•˜ê²Œ ë§ì¶°ì§€ëŠ” í˜„ìƒì„ ë°©ì§€í•œë‹¤.

---

### ğŸ”¹ ì •ê·œí™”ê°€ ì ìš©ëœ ê²½ì‚¬ í•˜ê°•ë²• (Gradient Descent with Regularization)

ë¹„ìš© í•¨ìˆ˜ì˜ í¸ë¯¸ë¶„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ  
ê°€ì¤‘ì¹˜ $w_j$ ì™€ í¸í–¥ $b$ ë¥¼ ì—…ë°ì´íŠ¸í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$
w_j = w_j - \alpha \frac{\partial}{\partial w_j} J(\vec{w},b)
$$

$$
b = b - \alpha \frac{\partial}{\partial b} J(\vec{w},b)
$$

---

### ğŸ”¹ ë¹„ìš© í•¨ìˆ˜ì˜ í¸ë¯¸ë¶„ (Partial Derivatives)

ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì— ëŒ€í•œ í¸ë¯¸ë¶„ ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.

$$
\frac{\partial}{\partial w_j} J(\vec{w},b)
= \frac{1}{m} \sum_{i=1}^m
\left( f_{\vec{w},b}(\vec{x}^i) - y^i \right)x_j^{(i)}
+ \frac{\lambda}{m} w_j
$$

$$
\frac{\partial}{\partial b} J(\vec{w},b)
= \frac{1}{m} \sum_{i=1}^m
\left( f_{\vec{w},b}(\vec{x}^i) - y^i \right)
$$

---

### ğŸ”¹ ì˜ë¯¸ ìš”ì•½

-   $w_j$ ì˜ ë¯¸ë¶„í•­ì—ëŠ” **ì •ê·œí™” í•­ $\frac{\lambda}{m} w_j$** ì´ ì¶”ê°€ëœë‹¤.
-   $b$ ëŠ” ì •ê·œí™” ëŒ€ìƒì´ ì•„ë‹ˆë¯€ë¡œ (í¸í–¥ì€ ëª¨ë¸ ë³µì¡ë„ì— ì§ì ‘ì  ì˜í–¥ì´ ì ìŒ),  
    **ë³€ê²½ë˜ì§€ ì•ŠëŠ”ë‹¤.**

---

### ğŸ”¹ ìµœì¢… ê²½ì‚¬ í•˜ê°•ë²• ì‹ (Final Regularized Gradient Descent)

$$
w_j = w_j - \alpha
\left[
\frac{1}{m} \sum_{i=1}^m
\left( f_{\vec{w},b}(\vec{x}^i) - y^i \right)x_j^{(i)}
+ \frac{\lambda}{m} w_j
\right]
$$

$$
b = b - \alpha
\frac{1}{m} \sum_{i=1}^m
\left( f_{\vec{w},b}(\vec{x}^i) - y^i \right)
$$

---

### ğŸ”¹ í•µì‹¬ ìš”ì•½

-   ì •ê·œí™” í•­ì€ $w$ ì—ë§Œ ì ìš©ë˜ë©°, $b$ ëŠ” ì˜í–¥ì„ ë°›ì§€ ì•ŠìŒ.
-   ì •ê·œí™” ê³„ìˆ˜ $\lambda$ ê°€ í¬ë©´ ëª¨ë¸ì´ ë‹¨ìˆœí•´ì§€ê³ , ì‘ìœ¼ë©´ ë³µì¡í•´ì§.
-   ê²½ì‚¬ í•˜ê°•ë²•ì˜ êµ¬ì¡°ëŠ” ë™ì¼í•˜ë‚˜,  
    ì˜¤ì§ $w_j$ ì˜ ì—…ë°ì´íŠ¸ ì‹ì— ì •ê·œí™” í•­ì´ ì¶”ê°€ëœë‹¤.

---

ğŸ‘‰ ìš”ì•½í•˜ìë©´,  
ì •ê·œí™”ëœ ì„ í˜• íšŒê·€ëŠ” ê¸°ë³¸ì ì¸ ì„ í˜• íšŒê·€ì—  
â€œíŒ¨ë„í‹°(penalty)â€ í•­ì„ ì¶”ê°€í•œ í˜•íƒœë¡œ,  
**ëª¨ë¸ì˜ ë³µì¡ë„ë¥¼ ì œì–´í•˜ë©° ê³¼ì í•©ì„ ë°©ì§€í•˜ëŠ” ì•ˆì •ì ì¸ í•™ìŠµ ë°©ì‹**ì´ë‹¤.

### Complete gradient descent of regularized linear regression

$$
w_j = w_j - \alpha \left[ \frac{1}{m} \sum_{i=1}^m \left[ (f_{\vec{w},b}(\vec{x}^i) - y^i)x_j^{(i)} \right] + \frac{\lambda}{m} w_j \right]
$$

$$
b = b - \alpha \frac{1}{m} \sum_{i=1}^m (f_{\vec{w},b}(\vec{x}^i) - y^i)
$$
